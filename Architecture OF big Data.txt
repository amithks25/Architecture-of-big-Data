>>mysql -uroot -pcloudera

*Creating Database for batting csv
>>create database battingdb;
>>use battingdb;



*Create batting table Schema
>>create table batting(playerID varchar(30),yearID int,stint int,teamID varchar(10),lgID varchar(10),G int,G_batting int,AB int,R int,H int,B2 int,B3 int,HR int,RBI int,SB int,CS int,BB int,SO int,IBB int,HBP int,SH int,SF int,GIDP int,G_old int);

*if yout data file have first row column name delete it and rename the file


*Loading Data into Table
>>Load data infile '/home/cloudera/Desktop/BDA/DataFiles/Batting1.csv' into table batting fields terminated by ',' Lines terminated by '\n';

*Sqoop the details into hdfs.
*outside the mysql 
>>sqoop import --connect jdbc:mysql://localhost/battingdb --username root --password cloudera --table batting --m 1

>>hadoop fs -cat /user/cloudera/batting/part-m-00000;

*Sqoop the details into hive.

*type command

>>hive;
>>create database battingdb;
>>use battingdb;
>>create table batting2(playerID STRING,yearID int,stint int,teamID STRING,lgID STRING,G int,G_batting int,AB int,R int,H int,B2 int,B3 int,HR int,RBI int,SB int,CS int,BB int,SO int,IBB int,HBP int,SH int,SF int,GIDP int,G_old int) row format delimited fields terminated by ',' stored as textfile;
>>LOAD DATA LOCAL INPATH '/home/cloudera/Desktop/BDA/DataFiles/Batting1.csv' into table batting;

#####note####
row format delimited fields terminated by ',' stored as textfile;
make sure u typed these commands while cretaing taable otherwise entire table is filled with null values.



******* PIG Questions**************

*Implement a PIG script to 
 a) Find the total count of participation of G 112

>>batting_list = LOAD '/home/cloudera/Desktop/BDA/DataFiles/Batting1.csv' USING PigStorage(',') as (playerID:chararray,yearID:int,stint:int,teamID:chararray,lgID:chararray,G:int,G_batting:int,AB:int,R:int,H:int,B2:int,B3:int,HR:int,RBI:int,SB:int,CS:int,BB:int,SO:int,IBB:int,HBP:int,SH:int,SF:int,GIDP:int,G_old:int);

>>dump batting_list;
*dump is equal to print command


>>count_g = FILTER batting_list BY G == 112;
>>group_count_g  = GROUP count_g All;
>>total_count = foreach group_count_g Generate COUNT(count_g.G);
>>dump total_count;
>>store total_count  into '/home/cloudera/Desktop/pig'; 

b) Find the player details with "david" ###

>>david  = Filter batting_list by(playerID MATCHES 'david.*');
>>dump david;

c) Find the average count of "NL" ###
>>NL_filter = Filter batting_list by lgID =='NL';
>>NL_Group = Group NL_filter All;
>>NL_avg = foreach NL_Group Generate AVG(NL_filter.G_batting);
>>DUMP NL_avg


d) Find the count of teams ###
>>team_count = GROUP batting_list by teamID;
>>team_group = GROUP team_count All;
>>result_count = Foreach team_group Generate COUNT(team_count);
>>dump result_count


*. Implement a Hive script to

* a) Find the total count of player details with "david"
>>select count(*) from batting where playerID REGEXP 'david[a-z]*';


*b) Create a patition on the TEAMID
*c) Create 3 buckets on the partition.

>>create external table batting_part(playerID string,yearID int,stint int,lgID string,G int,G_batting int,AB int,R int,H int,B2 int,B3 int,HR int,RBI int,SB int,CS int,BB int,SO int,IBB int,HBP int,SH int,SF int,GIDP int,G_old int)
partitioned by (teamID string)
clustered by (lgID) INTO 3 buckets
row format delimited
fields terminated by ','
stored as textfile;

>>create external table batting_hive(playerID string,yearID int,stint int,teamID string,lgID string,
G int,G_batting int,AB int,R int,H int,B2 int,B3 int,HR int,RBI int,SB int,CS int,BB int,SO int,IBB int,HBP int,SH int,SF int,GIDP int,G_old int)
row format delimited
fields terminated by ','
stored as textfile;


>>from batting_hive bat INSERT OVERWRITE TABLE batting_part PARTITION(teamID)
select bat.playerID,bat.yearID,
bat.stint ,bat.teamID,bat.lgID ,
bat.G ,bat.G_batting ,
bat.AB ,bat.R ,bat.H ,
bat.B2 ,bat.B3 ,
bat.HR ,bat.RBI ,
bat.SB ,bat.CS ,bat.BB ,
bat.SO ,bat.IBB, bat.HBP,
bat.SH ,bat.SF,bat.GIDP,
bat.G_old 
DISTRIBUTE BY teamID;


>>LOAD DATA LOCAL INPATH '/home/cloudera/Desktop/BDA/DataFiles/Batting.csv' OVERWRITE INTO TABLE batting_hive;


d) Extract the details on player "aaronha01" ###

>>select * from batting_part where playerID='aaronha01';



e) Find the count of teams ###

>>select count(distinct(teamID)) from batting_hive;
 

*9. Using hive,partition by year. Then, find the year wise count of participants, 
*find the total votes got by the players. ###

>>create table halloffame(hofID STRING, yearid INT, votedBy STRING, ballots INT, needed INT, votes INT,inducted STRING, category STRING, needed_note STRING) row format delimited fields terminated by ',' stored as textfile;



>>LOAD DATA LOCAL INPATH '/home/cloudera/Desktop/BDA/DataFiles/HallOfFame.csv' into table halloffame;



set hive.exec.dynamic.partition.mode=nonstrict;

set hive.exec.dynamic.partition=true;

set hive.enforce.bucketing=true;


>>create table halloffame_part(hofID STRING, votedBy STRING, ballots INT, needed INT, votes INT,inducted STRING, category STRING, needed_note STRING) partitioned by(yearid INT) row format delimited fields terminated by ',' lines terminated by '\n';


>>from halloffame hof INSERT OVERWRITE TABLE halloffame_part PARTITION(yearid) select hof.hofID, hof.votedBy, hof.ballots, hof.needed, hof.votes, hof.inducted, hof.category, hof.needed_note, hof.yearid  DISTRIBUTE BY yearid;


>>select yearid, count(hofid) from halloffame_part group by yearid;





*. Visualize the battings.csv based on the frequency of palyer inclusion yearwise.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df1 = dataset.groupby('yearID')['playerID'].count()
df1.head(5)

plt.figure(figsize=(15,10),dpi=100)
plt.plot(df1, linestyle='dotted', marker = '*', color = 'blue', label = 'Players')
plt.xlabel('Year')
plt.ylabel('Players Included')
plt.show()



*8. From halloffame.csv ###
*List the managers. ###
*List the managers.
from mrjob.job import MRJob



class MRmyjob(MRJob):
	def mapper(self,_,line):
		#split the line with tab separated fields
		data = line.split(',')
		hofid = data[0].strip()
		category = data[7].strip()
		if category == 'Manager':
			yield hofid,None
		
	

	def reducer(self, key, list_of_values):
		
		yield "manager",key
	
if __name__ == '__main__':
	MRmyjob.run();



